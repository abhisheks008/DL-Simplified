{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf Vectorization Model\n",
    "- TF-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.\n",
    "- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "Cosine Similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load the Goodreads dataset\n",
    "final_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Create a TF-IDF Vectorizer for the 'desc' column\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "\n",
    "\"\"\"To check Output from above code: \n",
    "# print(f\"Final Data Null Values: {final_data['Desc'].isnull().sum()}\")\n",
    "# print(f\"Lenght of Final Data: {len(final_data)}\")\n",
    "\n",
    "# print(f\"TfidfVectorizer: {tfidf_vectorizer}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "final_data['Desc'] = final_data['Desc'].fillna('')\n",
    "\n",
    "# Apply the TF-IDF vectorizer to the 'desc' column\n",
    "tfidf_matrix_desc = tfidf_vectorizer.fit_transform(final_data['Desc'])\n",
    "\n",
    "# print(f\"tfidf_matrix_desc: {tfidf_matrix_desc}\") # To check Output from above code\n",
    "# Convert the data type to float32\n",
    "tfidf_matrix_desc = tfidf_matrix_desc.astype(np.float32)\n",
    "\n",
    "# Compute the cosine similarity matrix for book descriptions\n",
    "cosine_sim_desc = linear_kernel(tfidf_matrix_desc, tfidf_matrix_desc)\n",
    "\n",
    "# Save the model as a pickle file\n",
    "if not os.path.exists('cosing_sim_desc.pkl'):\n",
    "    with open('cosing_sim_desc.pkl', 'wb') as f:\n",
    "        pickle.dump(cosine_sim_desc, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering (NCF) Model for Book Recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the Goodreads dataset\n",
    "final_data = pd.read_csv('GoodReads_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate User-Book Ratings\n",
    "Since the dataset does not contain user ratings, we simulate user-book interactions by generating random user IDs and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate user-book ratings\n",
    "# Assume 1000 users and assign random ratings between 1 and 5 to each book by users\n",
    "num_users = 1000\n",
    "num_ratings = len(final_data)\n",
    "\n",
    "# Generate random user IDs\n",
    "user_ids = np.random.randint(1, num_users + 1, num_ratings)\n",
    "\n",
    "# Generate random ratings\n",
    "ratings = np.random.randint(1, 6, num_ratings)\n",
    "\n",
    "# Add user IDs and ratings to the dataset\n",
    "final_data['user_id'] = user_ids\n",
    "final_data['rating'] = ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset for NCF\n",
    "We encode the user IDs and ISBNs as categorical variables to prepare the data for the NCF model. We then split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for NCF\n",
    "# Encode the user IDs and ISBNs\n",
    "final_data['user_id'] = final_data['user_id'].astype(\n",
    "    'category').cat.codes.values\n",
    "final_data['ISBN'] = final_data['ISBN'].astype('category').cat.codes.values\n",
    "\n",
    "# Select necessary columns for NCF\n",
    "user_item_data = final_data[['user_id', 'ISBN', 'rating']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(user_item_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NCF Model\n",
    "We define the NCF model architecture, which includes user and item embeddings, concatenation of these embeddings, and a series of dense layers to predict user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NCF model\n",
    "num_users = user_item_data['user_id'].nunique()\n",
    "num_items = user_item_data['ISBN'].nunique()\n",
    "\n",
    "# User input and embedding\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(\n",
    "    input_dim=num_users, output_dim=50, name='user_embedding')(user_input)\n",
    "user_vec = Flatten(name='user_flatten')(user_embedding)\n",
    "\n",
    "# Item input and embedding\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "item_embedding = Embedding(\n",
    "    input_dim=num_items, output_dim=50, name='item_embedding')(item_input)\n",
    "item_vec = Flatten(name='item_flatten')(item_embedding)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "concat = Concatenate(name='concat')([user_vec, item_vec])\n",
    "dense = Dense(128, activation='relu', name='dense1')(concat)\n",
    "dropout = Dropout(0.3, name='dropout')(dense)\n",
    "output = Dense(1, activation='linear', name='output')(dropout)\n",
    "\n",
    "# Compile the model\n",
    "model = Model([user_input, item_input], output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error', metrics=['mae']) # mae = Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Save the NCF Model\n",
    "We train the NCF model using the training data and evaluate its performance on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "921/921 [==============================] - 66s 68ms/step - loss: 2.6590 - mae: 1.3608 - val_loss: 1.9922 - val_mae: 1.2085\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 64s 69ms/step - loss: 0.9784 - mae: 0.8115 - val_loss: 2.1681 - val_mae: 1.2643\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 62s 67ms/step - loss: 0.5108 - mae: 0.5779 - val_loss: 2.0818 - val_mae: 1.2442\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 69s 74ms/step - loss: 0.2242 - mae: 0.3681 - val_loss: 2.0786 - val_mae: 1.2429\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 68s 74ms/step - loss: 0.1708 - mae: 0.3160 - val_loss: 2.0502 - val_mae: 1.2349\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 77s 84ms/step - loss: 0.1366 - mae: 0.2841 - val_loss: 2.0448 - val_mae: 1.2331\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 71s 77ms/step - loss: 0.1348 - mae: 0.2817 - val_loss: 2.0406 - val_mae: 1.2317\n",
      "Epoch 8/10\n",
      "921/921 [==============================] - 68s 74ms/step - loss: 0.1157 - mae: 0.2582 - val_loss: 2.0355 - val_mae: 1.2295\n",
      "Epoch 9/10\n",
      "921/921 [==============================] - 65s 71ms/step - loss: 0.1105 - mae: 0.2509 - val_loss: 2.0306 - val_mae: 1.2280\n",
      "Epoch 10/10\n",
      "921/921 [==============================] - 73s 79ms/step - loss: 0.1031 - mae: 0.2430 - val_loss: 2.0194 - val_mae: 1.2234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF Model Test MAE: 1.2234209775924683\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([train['user_id'], train['ISBN']], train['rating'],\n",
    "                    validation_data=(\n",
    "                        [test['user_id'], test['ISBN']], test['rating']),\n",
    "                    epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('ncf_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(\n",
    "    [test['user_id'], test['ISBN']], test['rating'], verbose=0)\n",
    "print(f'NCF Model Test MAE: {mae}')\n",
    "\n",
    "# Save the final_data to a CSV file\n",
    "final_data.to_csv(\"final_data_with_ratings.csv\", index=False)\n",
    "\n",
    "# Save the model architecture and weights\n",
    "model.save('ncf_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "#### Notes:\n",
    "\n",
    "* Data Preprocessing:\n",
    "\n",
    "We fill any missing descriptions with an empty string.\n",
    "Tokenize the text descriptions and convert them into sequences of integers.\n",
    "Pad these sequences to ensure they all have the same length for input into the CNN.\n",
    "\n",
    "* Simulate User-Book Interactions:\n",
    "\n",
    "Generate random user IDs and ratings to simulate user interactions with books.\n",
    "\n",
    "* Split the Data:\n",
    "\n",
    "Split the data into training and testing sets to evaluate the model's performance.\n",
    "\n",
    "* Define the CNN Model:\n",
    "\n",
    "The CNN model includes an embedding layer to convert word indices to dense vectors of fixed size.\n",
    "A Conv1D layer to apply convolutional operations on the text data.\n",
    "A GlobalMaxPooling1D layer to reduce the output size and capture the most important features.\n",
    "Dense layers to learn non-linear combinations of the features, and a dropout layer to prevent overfitting.\n",
    "Train and Evaluate the Model:\n",
    "\n",
    "Train the CNN model on the training data and evaluate its performance on the testing data.\n",
    "Save the trained model for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "921/921 [==============================] - 745s 805ms/step - loss: 2.2913 - mae: 1.2912 - val_loss: 2.0421 - val_mae: 1.2408\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 754s 819ms/step - loss: 2.0941 - mae: 1.2427 - val_loss: 2.0923 - val_mae: 1.2555\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 753s 817ms/step - loss: 1.7280 - mae: 1.1004 - val_loss: 2.2567 - val_mae: 1.2858\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 701s 761ms/step - loss: 0.9682 - mae: 0.7844 - val_loss: 2.6331 - val_mae: 1.3590\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 687s 746ms/step - loss: 0.6184 - mae: 0.6121 - val_loss: 2.5956 - val_mae: 1.3501\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 645s 700ms/step - loss: 0.4854 - mae: 0.5349 - val_loss: 2.5978 - val_mae: 1.3510\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 547s 594ms/step - loss: 0.4271 - mae: 0.4990 - val_loss: 2.5808 - val_mae: 1.3496\n",
      "Epoch 8/10\n",
      "921/921 [==============================] - 527s 572ms/step - loss: 0.4005 - mae: 0.4856 - val_loss: 2.6186 - val_mae: 1.3558\n",
      "Epoch 9/10\n",
      "921/921 [==============================] - 526s 571ms/step - loss: 0.3843 - mae: 0.4775 - val_loss: 2.5542 - val_mae: 1.3429\n",
      "Epoch 10/10\n",
      "921/921 [==============================] - 514s 559ms/step - loss: 0.3556 - mae: 0.4558 - val_loss: 2.5801 - val_mae: 1.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Test MAE: 1.3479958772659302\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Load the Goodreads dataset\n",
    "final_data = pd.read_csv('GoodReads_100k.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Fill NaN descriptions with an empty string\n",
    "final_data['Desc'] = final_data['Desc'].fillna('')\n",
    "\n",
    "# Tokenize the descriptions\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(final_data['Desc'])\n",
    "sequences = tokenizer.texts_to_sequences(final_data['Desc'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pad the sequences to ensure uniform input size\n",
    "max_length = 500  # You can choose an appropriate max length based on the data\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create user-book interactions for the model\n",
    "num_users = 1000\n",
    "num_ratings = len(final_data)\n",
    "\n",
    "# Generate random user IDs\n",
    "user_ids = np.random.randint(1, num_users + 1, num_ratings)\n",
    "\n",
    "# Generate random ratings\n",
    "ratings = np.random.randint(1, 6, num_ratings)\n",
    "\n",
    "# Add user IDs and ratings to the dataset\n",
    "final_data['user_id'] = user_ids\n",
    "final_data['rating'] = ratings\n",
    "\n",
    "# Encode the user IDs and ISBNs\n",
    "final_data['user_id'] = final_data['user_id'].astype('category').cat.codes.values\n",
    "final_data['ISBN'] = final_data['ISBN'].astype('category').cat.codes.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(final_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare input data for the CNN model\n",
    "X_train = padded_sequences[train.index]\n",
    "X_test = padded_sequences[test.index]\n",
    "y_train = train['rating'].values\n",
    "y_test = test['rating'].values\n",
    "\n",
    "# Define the CNN model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_length),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'CNN Model Test MAE: {mae}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Based Model\n",
    "\n",
    "1. Data Preprocessing:\n",
    "\n",
    "   * Fill any missing descriptions with an empty string.\n",
    "   * Tokenize the text descriptions using DistilBertTokenizer.\n",
    "\n",
    "2. Simulate User-Book Interactions:\n",
    "\n",
    "   * Generate random user IDs and ratings to simulate user interactions with books.\n",
    "\n",
    "3. Split the Data:\n",
    "\n",
    "   * Split the data into training and testing sets to evaluate the model's performance.\n",
    "\n",
    "4. Prepare the Tokenizer and Encode the Text:\n",
    "\n",
    "   * Tokenize the descriptions and create TensorFlow datasets for training and testing.\n",
    "\n",
    "5. Define the Transformer-based Model:\n",
    "\n",
    "   * Use a pre-trained DistilBERT model to encode the book descriptions.\n",
    "   * Define the model architecture with additional dense and dropout layers for prediction.\n",
    "\n",
    "6. Train and Evaluate the Model:\n",
    "\n",
    "   * Train the Transformer-based model on the training data and evaluate its performance on the testing data.\n",
    "   * Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "  24/3682 [..............................] - ETA: 73:04:19 - loss: 3.4622 - mae: 1.5500"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "def train_transformer_model():\n",
    "    \"\"\"\n",
    "    Train a transformer-based model for book recommendation.\n",
    "\n",
    "    This function loads the Goodreads dataset, preprocesses the data, generates random user-book ratings,\n",
    "    encodes the user IDs and ISBNs, splits the data into training and testing sets, prepares the tokenizer,\n",
    "    tokenizes the descriptions, converts the data to TensorFlow datasets, loads a pre-trained DistilBERT model,\n",
    "    defines a transformer-based model, compiles the model, trains the model, saves the model, and evaluates the model.\n",
    "\n",
    "    Returns:\n",
    "    - history: Training history of the model.\n",
    "    - loss: Loss value of the model on the test dataset.\n",
    "    - mae: Mean absolute error of the model on the test dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the Goodreads dataset\n",
    "    final_data = pd.read_csv('GoodReads_100k.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    final_data['Desc'] = final_data['Desc'].fillna('')\n",
    "\n",
    "    # Simulate user-book ratings\n",
    "    num_users = 1000\n",
    "    num_ratings = len(final_data)\n",
    "\n",
    "    # Generate random user IDs\n",
    "    user_ids = np.random.randint(1, num_users + 1, num_ratings)\n",
    "\n",
    "    # Generate random ratings\n",
    "    ratings = np.random.randint(1, 6, num_ratings)\n",
    "\n",
    "    # Add user IDs and ratings to the dataset\n",
    "    final_data['user_id'] = user_ids\n",
    "    final_data['rating'] = ratings\n",
    "\n",
    "    # Encode the user IDs and ISBNs\n",
    "    final_data['user_id'] = final_data['user_id'].astype('category').cat.codes.values\n",
    "    final_data['ISBN'] = final_data['ISBN'].astype('category').cat.codes.values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train, test = train_test_split(final_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Prepare the tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Tokenize the descriptions\n",
    "    train_encodings = tokenizer(train['Desc'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings = tokenizer(test['Desc'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Convert to TensorFlow datasets\n",
    "    def create_tf_dataset(encodings, labels):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(encodings),\n",
    "            labels\n",
    "        ))\n",
    "        return dataset\n",
    "\n",
    "    train_dataset = create_tf_dataset(train_encodings, train['rating'].values).batch(16)\n",
    "    test_dataset = create_tf_dataset(test_encodings, test['rating'].values).batch(16)\n",
    "\n",
    "    # Load the pre-trained DistilBERT model\n",
    "    transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Define the Transformer-based model\n",
    "    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    transformer_output = transformer_model(input_ids, attention_mask=attention_mask)\n",
    "    hidden_state = transformer_output.last_hidden_state\n",
    "    pooled_output = hidden_state[:, 0]\n",
    "\n",
    "    dense = Dense(128, activation='relu')(pooled_output)\n",
    "    dropout = Dropout(0.3)(dense)\n",
    "    output = Dense(1, activation='linear')(dropout)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_dataset, validation_data=test_dataset, epochs=1)\n",
    "\n",
    "    # Save the model\n",
    "    model.save('transformer_model.h5')\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, mae = model.evaluate(test_dataset, verbose=0)\n",
    "    print(f'Transformer Model Test MAE: {mae}')\n",
    "\n",
    "    return history, loss, mae\n",
    "\n",
    "history, loss, mae = train_transformer_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "f:\\Software\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Recommendations for 'Clojure Programming':\n",
      "1. The Human Equation: Building Profits by Putting People First\n",
      "2. Fashion Sourcebook 1920s\n",
      "3. All-American Anarchist: Joseph A. Labadie and the Labor Movement\n",
      "4. Hawaii: An Uncommon History\n",
      "5. Hungary 56\n",
      "6. Genuine Happiness: Meditation as the Path to Fulfillment\n",
      "7. Anarchism And Ecology\n",
      "8. Anthropological Studies of Religion: An Introductory Text\n",
      "9. Competitive Advantage Through People: Unleashing the Power of the Work Force\n",
      "10. Between Two Fires: American Indians in the Civil War\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Load the final data\n",
    "df = pd.read_csv('final_data_with_ratings.csv')\n",
    "\n",
    "# load the model\n",
    "cosine_sim = pickle.load(open('cosing_sim_desc.pkl', 'rb'))\n",
    "cnn_model = tf.keras.models.load_model('cnn_model.h5')\n",
    "ncf_model = tf.keras.models.load_model('ncf_model.h5')\n",
    "\n",
    "def hybrid_recommendation(book_name, top_n=10):\n",
    "    # Cosine Similarity recommendations\n",
    "    idx = df[df['Title'].str.contains(book_name, case=False, na=False)].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    cosine_recs = [i[0] for i in sim_scores[1:top_n+1]]\n",
    "    \n",
    "    # NCF recommendations (dummy user for demo purposes)\n",
    "    user_input = np.array([0] * top_n)\n",
    "    item_input = np.array(cosine_recs)\n",
    "    ncf_preds = ncf_model.predict([user_input, item_input]).flatten()\n",
    "    \n",
    "    # CNN recommendations\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(df['Title'])\n",
    "    text_input = tokenizer.texts_to_sequences([book_name] * top_n)\n",
    "    text_input = pad_sequences(text_input, maxlen=100)\n",
    "    cnn_preds = cnn_model.predict(text_input).flatten()\n",
    "    \n",
    "    # Aggregate and rank recommendations\n",
    "    combined_scores = ncf_preds + cnn_preds\n",
    "    recommended_indices = np.argsort(combined_scores)[-top_n:]\n",
    "    \n",
    "    recommended_books = df.iloc[recommended_indices]['Title'].values\n",
    "    return recommended_books\n",
    "\n",
    "# Example usage\n",
    "book_name = \"Clojure Programming\"\n",
    "recommendations = hybrid_recommendation(book_name)\n",
    "print(f\"Recommendations for '{book_name}':\")\n",
    "for i, book in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {book}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
